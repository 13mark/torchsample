{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What are Samplers in Pytorch?\n",
    "\n",
    "Samplers determine how each individual image (or subjecet) will be fed to the network during a single epoch. There are two main samplers built-in to  `torchvision`/`pytorch`. They are called `SequentialSampler` and `RandomSampler` (produced below). They do exactly what you think they do - sample your data in order, or shuffle each sample before returning them.\n",
    "\n",
    "```python\n",
    "class SequentialSampler(Sampler):\n",
    "    def __init__(self, nb_samples):\n",
    "        self.num_samples = nb_samples\n",
    "\n",
    "    def __iter__(self):\n",
    "        return iter(range(self.num_samples))\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.num_samples\n",
    "\n",
    "\n",
    "class RandomSampler(Sampler):\n",
    "    def __init__(self, nb_samples):\n",
    "        self.num_samples = nb_samples\n",
    "\n",
    "    def __iter__(self):\n",
    "        return iter(torch.randperm(self.num_samples).long())\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.num_samples\n",
    "```\n",
    "\n",
    "You'll see how they operated on integers. That's how sampling works in pytorch - each individual image/subject/whatever is accessed using an integer index.\n",
    "\n",
    "Let's demonstrate their usage. Starting with the default (i.e. doing nothing):\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "x = torch.ones(3,1,10,10) # three samples of (1,10,10) size\n",
    "y = torch.from_numpy(np.arange(3)) # class labels = (1,2,3)\n",
    "\n",
    "from torchsample import TensorDataset\n",
    "data = TensorDataset(x,y)\n",
    "for i,j in data:\n",
    "    print(j[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok, obvious - by default the torchsample datasets will take samples in the order they are given. We can change this by passing `shuffle=True` into the class:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n",
      "0\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "data = TensorDataset(x,y, shuffle=True)\n",
    "for i, j in data:\n",
    "    print(j[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cool, our data has now been shuffled. However, we can use samplers explicitly to provide more nuanced sampling in our data. Let's start by recreating the above two examples using the actual samplers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using Sequential Sampler:\n",
      "0\n",
      "1\n",
      "2\n",
      "Using Random Sampler:\n",
      "2\n",
      "1\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "from torchsample.samplers import RandomSampler, SequentialSampler\n",
    "\n",
    "rs = RandomSampler(nb_samples=3)\n",
    "ss = SequentialSampler(nb_samples=3)\n",
    "\n",
    "sequential_data = TensorDataset(x,y, sampler=ss)\n",
    "print('Using Sequential Sampler:')\n",
    "for i, j in sequential_data:\n",
    "    print(j[0])\n",
    "    \n",
    "random_data = TensorDataset(x,y, sampler=rs)\n",
    "print('Using Random Sampler:')\n",
    "for i, j in random_data:\n",
    "    print(j[0])\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That was straight-forward. Now, let's look at the first (of many to come) enhanced samplers. This next sampler is called `MultiSampler`, because it lets you take more samples in an epoch than there actually are samples in the data. This is incredibly useful if you're training on 2D slices from 3D images or even crops which are much smaller than the total image size.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "0\n",
      "1\n",
      "2\n",
      "0\n",
      "1\n",
      "2\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "from torchsample.samplers import MultiSampler\n",
    "\n",
    "ms = MultiSampler(nb_samples=3, desired_samples=10) # we have 3 samples, but we want 10 samples per epoch\n",
    "multi_data = TensorDataset(x, y, sampler=ms)\n",
    "for i, j in multi_data:\n",
    "    print(j[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Above, we essentially got 10 samples out of our data even though there are only three actual samples. You can see the routine here -- loop over the data in order for whatever `floor(desired_samples / nb_samples)` is, then for the difference just randomly take more samples from the entire pool.\n",
    "\n",
    "You can also use this sampler to take <i>less</i> samples than exist in your data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "ms = MultiSampler(nb_samples=3, desired_samples=2) # we have 3 samples, but we want 10 samples per epoch\n",
    "multi_data = TensorDataset(x, y, sampler=ms)\n",
    "for i, j in multi_data:\n",
    "    print(j[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we just took two samples at random.\n",
    "\n",
    "Samplers allow you to implement any type of sampling procedure you like, while abstracting away any data handling. This is a nice thing. It would be nice to have the stratified sampling as in scikit-learn. Maybe that's next on the sampler to-do list."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, one last thing to note is that you can actually sample indefinitely from datasets in `torchsample` by calling the `next()` method on the data. This returns an iterator just like the for loops above, but will automatically reset the iterator after you've reached the end of the data. This can be useful for someone I'm sure:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "0\n",
      "1\n",
      "2\n",
      "0\n",
      "1\n",
      "2\n",
      "0\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "data = TensorDataset(x, y, shuffle=False)\n",
    "count = 0\n",
    "while True:\n",
    "    i, j = data.next()\n",
    "    print(j[0])\n",
    "    count+=1\n",
    "    if count > 10:\n",
    "        break # break so we dont loop forever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
